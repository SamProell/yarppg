{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the yarPPG documentation","text":"<p>yarPPG is yet another implementation of remote PhotoPlethysmoGraphy. Remote photo\u00adplethysmography (rPPG) refers to the camera-based measurement of a blood volume pulse signal. It works by detecting small changes in skin color, originating from the pulsation of blood<sup>1</sup>.</p> <p>Warning</p> <p>This is just a hobby project. Intended for demo purposes only, the provided program/code is not suitable to be used in a clinical setup or for any decision making in general.</p>"},{"location":"#installation-and-usage","title":"Installation and usage","text":"<p>In order to run the yarPPG application, clone the repository and navigate to the downloaded folder. You can then install the folder into your Python environment. This will install the <code>run-yarppg</code> command.</p> <pre><code>git clone https://github.com/SamProell/yarppg.git\ncd yarppg\npip install \".\"\nrun-yarppg\n</code></pre> <p></p>"},{"location":"#core-functionality","title":"Core functionality","text":"<p>Different from earlier versions of yarPPG, the core functionality for remote PPG signal extraction has been completely decoupled from the user interface. The <code>Rppg</code> class combines all required steps (roi identification, signal extraction, heart rate estimation) into one (stateful) function.</p> <pre><code>import yarppg\n\nrppg = yarppg.Rppg()\n\nwhile running:\n    # frame = ...  # get an image array of shape h x w x 3\n    result = rppg.process_frame(frame)\n    print(f\"Current rPPG signal value: {result.value} (HR: {result.hr})\")\n</code></pre> <p>See this guide if you need more fine-grained control over the individual calculation steps. The <code>Rppg</code> class also comes with a method to process an entire video file at once. See more details here.</p>"},{"location":"#user-interfaces","title":"User interfaces","text":"<p>The default user interface launched by the <code>run-yarppg</code> command is a simplistic window based on OpenCV. More elaborate user interfaces are available, but require additional dependencies.</p>"},{"location":"#simple-qt6-window","title":"Simple Qt6 window","text":"<pre><code>pip install \".[qt6]\"\nrun-yarppg ui=qt6_simple\n</code></pre>"},{"location":"#more-to-come","title":"More to come","text":"<p>You are welcome to contribute</p> <ol> <li> <p>W Verkruysse, L O Svaasand and J S Nelson. Remote plethysmographic imaging using ambient light. Optics Express. 2008;16(26):21434\u201321445. doi:10.1364/oe.16.021434 \u21a9</p> </li> </ol>"},{"location":"cli/","title":"Command line interface","text":"<p>yarPPG comes with the <code>run-yarppg</code> command to launch a graphical user interface. The command line interface is built as a structured configuration using Hydra.</p> <p>Hydra offers a robust configuration management with type checking for complex, modular settings hierarchies. Additionally, we get a powerful override syntax, allowing users to adjust the settings from the command line.</p> <p>Note</p> <p>The command line interface is still a work in progress. For now, you can adjust only a handful of options.</p> <p>You can call <code>run-yarppg --help</code> to get more information of available options and how to override them:</p> <pre><code>run-yarppg is powered by Hydra.\n\n== Configuration groups ==\nCompose your configuration from those groups (group=option)\n\nui: qt6_simple, simplest\n\n\n== Config ==\nOverride anything in the config (foo.bar=value)\n\nui:\n  roi_alpha: 0.0\n  video: 0\nsavepath: null\ndetector: facemesh\nfilter:\n  fs: 30.0\n  f1: 0.5\n  f2: 2.0\n  btype: bandpass\n  ftype: butter\n  order: 2\nalgorithm: green\n\n\nPowered by Hydra (https://hydra.cc)\nUse --hydra-help to view Hydra specific help\n</code></pre>"},{"location":"deepdive/","title":"Deep dive","text":"In\u00a0[1]: Copied! <pre>import matplotlib.patches\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport yarppg\n\nfilename = \"tests/testvideo_30fps.mp4\"\nfps = yarppg.helpers.get_video_fps(filename)\n</pre> import matplotlib.patches import matplotlib.pyplot as plt import numpy as np  import yarppg  filename = \"tests/testvideo_30fps.mp4\" fps = yarppg.helpers.get_video_fps(filename) In\u00a0[2]: Copied! <pre>rppg = yarppg.Rppg()\n</pre> rppg = yarppg.Rppg() <p>If you are only interested in the results, you can do something like this:</p> In\u00a0[3]: Copied! <pre>results: list[yarppg.RppgResult] = []\nfor frame in yarppg.frames_from_video(filename):\n    results.append(rppg.process_frame(frame))\nplt.plot(np.array(results)[:, 0])  # plot rPPG signal\n</pre> results: list[yarppg.RppgResult] = [] for frame in yarppg.frames_from_video(filename):     results.append(rppg.process_frame(frame)) plt.plot(np.array(results)[:, 0])  # plot rPPG signal Out[3]: <pre>[&lt;matplotlib.lines.Line2D at 0x25e40497350&gt;]</pre> <p>Under the hood, <code>rppg.process_frame</code> calls</p> <ol> <li>a roi detector's <code>detect</code> method</li> <li>a signal extracor's (<code>yarppg.Processor</code>) <code>process</code></li> <li>a <code>HrCalculator</code>s <code>update</code> method.</li> </ol> <p>Let's define each component separately.</p> In\u00a0[4]: Copied! <pre>roi_detector = yarppg.FaceMeshDetector()\n\nframe = next(yarppg.frames_from_video(filename))\nroi = roi_detector.detect(frame)\nplt.imshow(roi.mask &gt; 0, cmap=\"Greys_r\", aspect=\"auto\")\n\nassert (\n    roi.face_rect is not None\n)  # FaceMeshDetector also provides a bounding box.\nx, y, w, h = roi.face_rect\nrect = matplotlib.patches.Rectangle(\n    (x, y), w, h, edgecolor=\"r\", facecolor=\"none\"\n)\nplt.gca().add_patch(rect)\nplt.axis(\"off\")\n</pre> roi_detector = yarppg.FaceMeshDetector()  frame = next(yarppg.frames_from_video(filename)) roi = roi_detector.detect(frame) plt.imshow(roi.mask &gt; 0, cmap=\"Greys_r\", aspect=\"auto\")  assert (     roi.face_rect is not None )  # FaceMeshDetector also provides a bounding box. x, y, w, h = roi.face_rect rect = matplotlib.patches.Rectangle(     (x, y), w, h, edgecolor=\"r\", facecolor=\"none\" ) plt.gca().add_patch(rect) plt.axis(\"off\") Out[4]: <pre>(np.float64(-0.5), np.float64(1919.5), np.float64(1079.5), np.float64(-0.5))</pre> In\u00a0[5]: Copied! <pre>processor = yarppg.Processor()\nresult = processor.process(roi)\nprint(result.value == result.roi_mean.g)\n</pre> processor = yarppg.Processor() result = processor.process(roi) print(result.value == result.roi_mean.g) <pre>True\n</pre> In\u00a0[6]: Copied! <pre>hrcalc = yarppg.PeakBasedHrCalculator(\n    fs=30, window_seconds=5, distance=0.6, update_interval=15\n)\n</pre> hrcalc = yarppg.PeakBasedHrCalculator(     fs=30, window_seconds=5, distance=0.6, update_interval=15 ) <p>We can see the internal buffer growing, when repeatedly calling <code>update</code>. Note that HR will be nan as long as the buffer is smaller than the expected window size. To clear the buffer, we call <code>hrcalc.reset()</code>.</p> In\u00a0[7]: Copied! <pre>for res in results[:5]:\n    hr = hrcalc.update(res.value)\n    print(\"Buffer lenght:\", len(hrcalc.values), \"HR:\", hr)\nhrcalc.reset()\nprint(\"Buffer lenght:\", len(hrcalc.values), \"- state cleared.\")\n</pre> for res in results[:5]:     hr = hrcalc.update(res.value)     print(\"Buffer lenght:\", len(hrcalc.values), \"HR:\", hr) hrcalc.reset() print(\"Buffer lenght:\", len(hrcalc.values), \"- state cleared.\") <pre>Buffer lenght: 1 HR: nan\nBuffer lenght: 2 HR: nan\nBuffer lenght: 3 HR: nan\nBuffer lenght: 4 HR: nan\nBuffer lenght: 5 HR: nan\nBuffer lenght: 0 - state cleared.\n</pre> In\u00a0[8]: Copied! <pre># Clear the previous state.\nprocessor.reset()\nhrcalc.reset()\n\nresults: list[yarppg.RppgResult] = []\nfor i, frame in enumerate(yarppg.frames_from_video(filename)):\n    roi = roi_detector.detect(frame)\n    result = processor.process(roi)\n    result.hr = hrcalc.update(result.value)\n\n    results.append(result)\n    if i % 30 == 0:\n        print(\n            f\"{i=} {(roi.mask &gt; 0).mean()=:.1%} {result.value=:.2f}\"\n            f\" {result.hr=:.2f}\"\n        )\n\nplt.plot(np.array(results)[:, 0])\n</pre> # Clear the previous state. processor.reset() hrcalc.reset()  results: list[yarppg.RppgResult] = [] for i, frame in enumerate(yarppg.frames_from_video(filename)):     roi = roi_detector.detect(frame)     result = processor.process(roi)     result.hr = hrcalc.update(result.value)      results.append(result)     if i % 30 == 0:         print(             f\"{i=} {(roi.mask &gt; 0).mean()=:.1%} {result.value=:.2f}\"             f\" {result.hr=:.2f}\"         )  plt.plot(np.array(results)[:, 0]) <pre>i=0 (roi.mask &gt; 0).mean()=3.6% result.value=120.17 result.hr=nan\n</pre> <pre>i=30 (roi.mask &gt; 0).mean()=3.9% result.value=121.97 result.hr=nan\n</pre> <pre>i=60 (roi.mask &gt; 0).mean()=3.8% result.value=122.37 result.hr=nan\n</pre> <pre>i=90 (roi.mask &gt; 0).mean()=3.9% result.value=122.00 result.hr=nan\n</pre> <pre>i=120 (roi.mask &gt; 0).mean()=3.9% result.value=122.29 result.hr=nan\n</pre> <pre>i=150 (roi.mask &gt; 0).mean()=3.9% result.value=122.70 result.hr=23.67\n</pre> <pre>i=180 (roi.mask &gt; 0).mean()=3.9% result.value=121.92 result.hr=25.20\n</pre> <pre>i=210 (roi.mask &gt; 0).mean()=3.8% result.value=121.19 result.hr=23.40\n</pre> <pre>i=240 (roi.mask &gt; 0).mean()=3.9% result.value=121.87 result.hr=25.80\n</pre> <pre>i=270 (roi.mask &gt; 0).mean()=3.8% result.value=122.05 result.hr=26.60\n</pre> Out[8]: <pre>[&lt;matplotlib.lines.Line2D at 0x25e405f9750&gt;]</pre>"},{"location":"deepdive/#diving-deeper-into-the-rppg-components","title":"Diving deeper into the rPPG components\u00b6","text":"<p>This guide walks you through the inner workings of the <code>Rppg.process_frame</code> method.</p>"},{"location":"deepdive/#overview","title":"Overview\u00b6","text":"<p>Remote Photoplethysmography (rPPG) typically involves three steps:</p> <ul> <li>region of interest (ROI) identification</li> <li>signal extraction</li> <li>heart rate estimation</li> </ul> <p>The yarppg.Rppg class combines all these steps in a convenient manner. By default, <code>yarppg.Rppg()</code> will give you a simple processor, that finds the lower part of a face and extracts the average green channel within the region.</p>"},{"location":"deepdive/#region-of-interest-detection","title":"Region of interest detection\u00b6","text":"<p>yarPPG comes with several different implementations of ROI detectors. By default, we use an AI-based face landmarker provided through Google's MediaPipe. The <code>FaceMeshDetector</code> applies the face landmarker model and extracts the region of the lower face, as is done by Li et al. (2014).</p> <p>X. Li, J. Chen, G. Zhao, and M. Pietikainen, \u201cRemote Heart Rate Measurement From Face Videos Under Realistic Situations\u201d, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4264-4271, 2014 doi:10.1109/CVPR.2014.543</p> <p>We can visualize the ROI mask, which is &gt; 0 for each pixel of the ROI. Some segmenters, including the FaceMeshDetector, also return the bounding box of the detected face. We mark the bounding box as a red rectangle below.</p>"},{"location":"deepdive/#signal-extraction","title":"Signal extraction\u00b6","text":"<p>The default signal extractor (<code>yarppg.Processor</code>) simply calculates the average green channel within the region of interest. This can already be enough to estimate heart rate accurately, if there is no movement and no lighting changes.</p> <p>The processor returns an <code>RppgResult</code> container, which includes some additional information besides the extracted value. For example, all processors return the mean color (R, G and B) of the ROI, regardless of the specific algorithm.</p>"},{"location":"deepdive/#heart-rate-estimation","title":"Heart rate estimation\u00b6","text":"<p>In order to perform heart rate estimation, we need to look at the rPPG signal over time. The <code>yarppg.HrCalculator</code> keeps an internal buffer of recent signal values and periodically updates the heart rate estimate. The <code>PeakBasedHrCalculator</code> identifies peaks in the rPPG signal and calculates heart rate from the average distance of peaks within the buffer window.</p> <p>In your signal processing loop, you can call the <code>update</code> method in every iteration. The calculator will decide based on the <code>update_interval</code> attribute, whether to perform the calculation or to simply store the value in the buffer. Below, we set up the calculator to produce a new HR estimate with every 15th frame.</p>"},{"location":"deepdive/#putting-everything-together","title":"Putting everything together\u00b6","text":"<p>We can combine all of the above tools to build a fully customizable and extendable rPPG processing loop (see the [<code>yarppg.ui.simplest</code>][yarppg.ui.simplest] loop for an equivalent implementation with a simplistic UI.)</p>"},{"location":"profile_processing/","title":"Profile processing","text":"In\u00a0[1]: Copied! <pre>\"\"\"Profile the offline video processing.\"\"\"\n</pre> \"\"\"Profile the offline video processing.\"\"\" Out[1]: <pre>'Profile the offline video processing.'</pre> In\u00a0[2]: Copied! <pre>#! %load_ext cProfile\nimport yarppg\n</pre> #! %load_ext cProfile import yarppg In\u00a0[3]: Copied! <pre>filename = \"tests/testvideo_30fps.mp4\"\n\nfps = yarppg.helpers.get_video_fps(filename)\nfilter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\")\nlivefilter = yarppg.digital_filter.make_digital_filter(filter_cfg)\nprocessor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter)\nrppg = yarppg.Rppg(\n    processor=processor,\n    hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4),\n)\n</pre> filename = \"tests/testvideo_30fps.mp4\"  fps = yarppg.helpers.get_video_fps(filename) filter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\") livefilter = yarppg.digital_filter.make_digital_filter(filter_cfg) processor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter) rppg = yarppg.Rppg(     processor=processor,     hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4), ) In\u00a0[4]: Copied! <pre>#!%prun -D process_video.prof rppg.process_video(filename)\n</pre> #!%prun -D process_video.prof rppg.process_video(filename)"},{"location":"video_processing/","title":"Video processing","text":"<p>yarPPG can now also be applied fully offline, without a user interface. To streamline such use cases, the <code>Rppg</code> orchestrator provides a helper function to process a video file in one line: <code>process_video</code>.</p> In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nimport yarppg\n</pre> import matplotlib.pyplot as plt import numpy as np  import yarppg In\u00a0[2]: Copied! <pre>filename = \"tests/testvideo_30fps.mp4\"\n\nfps = yarppg.get_video_fps(filename)\nfilter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\")\nlivefilter = yarppg.digital_filter.make_digital_filter(filter_cfg)\nprocessor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter)\n</pre> filename = \"tests/testvideo_30fps.mp4\"  fps = yarppg.get_video_fps(filename) filter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\") livefilter = yarppg.digital_filter.make_digital_filter(filter_cfg) processor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter) <p>Since the example video is quite short, we modify the behavior of the heart rate calculator, to produce an update with a window length of only four seconds. In practice, this will result in less accurate estimations, as outliers contribute more to the final result. Additionally, since the processors need a few seconds to adjust to the specific video content (lighting, colors, etc.), the provided estimates for this 10s video are of low quality.</p> In\u00a0[3]: Copied! <pre>rppg = yarppg.Rppg(\n    processor=processor,\n    hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4),\n)\n</pre> rppg = yarppg.Rppg(     processor=processor,     hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4), ) In\u00a0[4]: Copied! <pre>results = rppg.process_video(filename)\n</pre> results = rppg.process_video(filename) In\u00a0[5]: Copied! <pre>values = np.array(results)\nhrs = yarppg.bpm_from_frames_per_beat(values[:, -1], fps)\nplt.plot(values[int(2.5 * fps) :, 0])\nplt.twinx().plot(hrs[int(2.5 * fps) :], \"C2\")\n</pre> values = np.array(results) hrs = yarppg.bpm_from_frames_per_beat(values[:, -1], fps) plt.plot(values[int(2.5 * fps) :, 0]) plt.twinx().plot(hrs[int(2.5 * fps) :], \"C2\") Out[5]: <pre>[&lt;matplotlib.lines.Line2D at 0x26f00a75a50&gt;]</pre>"},{"location":"video_processing/#setup","title":"Setup\u00b6","text":"<p>For this demo, we set up a bandpass-filtered version of the default processor, which extracts the average green channel of the region of interest. We need to know the number of frames per second (FPS) of the video file, to properly set up the filter. <code>yarppg.get_video_fps</code> uses OpenCV to extract the FPS information from the video file.</p>"},{"location":"video_processing/#processing-the-video","title":"Processing the video\u00b6","text":"<p>Once setup, we can process the video in just one line. By default, <code>process_video</code> returns a list of <code>RppgResult</code> containers. Beware that these include the raw image data from all frames and ROI masks inside the <code>RegionOfInterest</code> container.</p>"},{"location":"video_processing/#handling-results","title":"Handling results\u00b6","text":"<p><code>RppgResult</code> allows easy conversion to an array. Even the list of results can be converted neatly. This produces an Nx8 array with the following values for each frame:</p> <pre><code>value, roi_r, roi_g, roi_b, bg_r, bg_g, bg_b, hr\n</code></pre> <p>.</p>"},{"location":"reference/","title":"Reference","text":"<p>The following pages provide a more detailed look into all elements of the yarPPG code.</p> <ul> <li>The rPPG orchestrator combines all steps required to perform rPPG</li> <li>ROI detection gives an overview of available ROI detectors</li> <li>rPPG processors lists available rPPG processors</li> <li>Heart rate estimation describes heart rate estimators</li> <li>Data containers are used to bundle intermediate results meaningfully</li> </ul>"},{"location":"reference/containers/","title":"Data containers","text":"<p>Defines some containers passed between objects of the yarPPG application.</p>"},{"location":"reference/containers/#yarppg.containers.Color","title":"<code>Color</code>  <code>dataclass</code>","text":"<p>Defines a color in RGB(A) format.</p> Source code in <code>src\\yarppg\\containers.py</code> <pre><code>@dataclass\nclass Color:\n    \"\"\"Defines a color in RGB(A) format.\"\"\"\n\n    r: float\n    g: float\n    b: float\n\n    @classmethod\n    def null(cls):\n        \"\"\"Create empty color with NaN values.\"\"\"\n        return cls(np.nan, np.nan, np.nan)\n\n    def __array__(self):\n        return np.array([self.r, self.g, self.b])\n\n    @classmethod\n    def from_array(cls, arr: np.ndarray):\n        \"\"\"Convert numpy array to `Color` object.\"\"\"\n        if len(arr) in {3, 4} and arr.ndim == 1:\n            return cls(*arr)\n        raise ValueError(f\"Cannot interpret {arr=!r}\")\n</code></pre>"},{"location":"reference/containers/#yarppg.containers.Color.from_array","title":"<code>from_array(arr)</code>  <code>classmethod</code>","text":"<p>Convert numpy array to <code>Color</code> object.</p> Source code in <code>src\\yarppg\\containers.py</code> <pre><code>@classmethod\ndef from_array(cls, arr: np.ndarray):\n    \"\"\"Convert numpy array to `Color` object.\"\"\"\n    if len(arr) in {3, 4} and arr.ndim == 1:\n        return cls(*arr)\n    raise ValueError(f\"Cannot interpret {arr=!r}\")\n</code></pre>"},{"location":"reference/containers/#yarppg.containers.Color.null","title":"<code>null()</code>  <code>classmethod</code>","text":"<p>Create empty color with NaN values.</p> Source code in <code>src\\yarppg\\containers.py</code> <pre><code>@classmethod\ndef null(cls):\n    \"\"\"Create empty color with NaN values.\"\"\"\n    return cls(np.nan, np.nan, np.nan)\n</code></pre>"},{"location":"reference/containers/#yarppg.containers.RegionOfInterest","title":"<code>RegionOfInterest</code>  <code>dataclass</code>","text":"<p>Container for defining the region of interest (and background) in an image.</p> Source code in <code>src\\yarppg\\containers.py</code> <pre><code>@dataclass\nclass RegionOfInterest:\n    \"\"\"Container for defining the region of interest (and background) in an image.\"\"\"\n\n    mask: np.ndarray\n    baseimg: np.ndarray\n    bg_mask: np.ndarray | None = None\n    face_rect: tuple[int, int, int, int] | None = None\n    \"\"\"Bounding box of the detected face (x, y, w, h).\"\"\"\n</code></pre>"},{"location":"reference/containers/#yarppg.containers.RegionOfInterest.face_rect","title":"<code>face_rect: tuple[int, int, int, int] | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bounding box of the detected face (x, y, w, h).</p>"},{"location":"reference/containers/#yarppg.containers.RppgResult","title":"<code>RppgResult</code>  <code>dataclass</code>","text":"<p>Container for rPPG computation results.</p> <p>Calling <code>np.array</code> on this container will return a 8-element vector containing the rPPG signal value, RGB values of the ROI, RGB values of the background (or nans) and the HR. <code>to_series</code> produces a clearer representation of the values with named indices.</p> <p>Note that both <code>__array__</code> and <code>to_series</code> ignore the <code>roi</code> attribute.</p> Source code in <code>src\\yarppg\\containers.py</code> <pre><code>@dataclass\nclass RppgResult:\n    \"\"\"Container for rPPG computation results.\n\n    Calling `np.array` on this container will return a 8-element vector containing\n    the rPPG signal value, RGB values of the ROI, RGB values of the background (or nans)\n    and the HR. `to_series` produces a clearer representation of the values with named\n    indices.\n\n    Note that both `__array__` and `to_series` ignore the `roi` attribute.\n    \"\"\"\n\n    value: float\n    \"\"\"Output value of the rPPG signal extractor.\"\"\"\n    roi: RegionOfInterest\n    \"\"\"Region of interest identified in the current frame.\"\"\"\n    roi_mean: Color\n    \"\"\"Mean color of the ROI.\"\"\"\n    bg_mean: Color\n    \"\"\"Mean color of the background.\"\"\"\n    hr: float = np.nan\n    \"\"\"Heart rate estimate in frames per beat.\"\"\"\n\n    def __array__(self):\n        return np.r_[self.value, self.roi_mean, self.bg_mean, self.hr]\n\n    def to_series(self):\n        \"\"\"Extract the rPPG signal values into a Pandas series.\"\"\"\n        return pd.Series(\n            np.array(self),\n            index=[\"value\", \"roi_r\", \"roi_g\", \"roi_b\", \"bg_r\", \"bg_g\", \"bg_b\", \"hr\"],\n        )\n</code></pre>"},{"location":"reference/containers/#yarppg.containers.RppgResult.bg_mean","title":"<code>bg_mean: Color</code>  <code>instance-attribute</code>","text":"<p>Mean color of the background.</p>"},{"location":"reference/containers/#yarppg.containers.RppgResult.hr","title":"<code>hr: float = np.nan</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Heart rate estimate in frames per beat.</p>"},{"location":"reference/containers/#yarppg.containers.RppgResult.roi","title":"<code>roi: RegionOfInterest</code>  <code>instance-attribute</code>","text":"<p>Region of interest identified in the current frame.</p>"},{"location":"reference/containers/#yarppg.containers.RppgResult.roi_mean","title":"<code>roi_mean: Color</code>  <code>instance-attribute</code>","text":"<p>Mean color of the ROI.</p>"},{"location":"reference/containers/#yarppg.containers.RppgResult.value","title":"<code>value: float</code>  <code>instance-attribute</code>","text":"<p>Output value of the rPPG signal extractor.</p>"},{"location":"reference/containers/#yarppg.containers.RppgResult.to_series","title":"<code>to_series()</code>","text":"<p>Extract the rPPG signal values into a Pandas series.</p> Source code in <code>src\\yarppg\\containers.py</code> <pre><code>def to_series(self):\n    \"\"\"Extract the rPPG signal values into a Pandas series.\"\"\"\n    return pd.Series(\n        np.array(self),\n        index=[\"value\", \"roi_r\", \"roi_g\", \"roi_b\", \"bg_r\", \"bg_g\", \"bg_b\", \"hr\"],\n    )\n</code></pre>"},{"location":"reference/helpers/","title":"Helper functions","text":"<p>Utility functions and helpers.</p>"},{"location":"reference/helpers/#yarppg.helpers.FpsTracker","title":"<code>FpsTracker</code>","text":"<p>Utility class to track frames per second.</p> <p>Use <code>tracker.tick()</code> once per update (e.g., per frame). The tracker stores the time differences (dt) between successive <code>tick</code> calls. You can then get the current estimate of FPS through the <code>tracker.fps</code> property.</p> <p>Parameters:</p> Name Type Description Default <code>maxlen</code> <p>number of time differences to use for FPS calculation. Defaults to 30.</p> <code>30</code> Source code in <code>src\\yarppg\\helpers.py</code> <pre><code>class FpsTracker:\n    \"\"\"Utility class to track frames per second.\n\n    Use `tracker.tick()` once per update (e.g., per frame). The tracker\n    stores the time differences (dt) between successive `tick` calls.\n    You can then get the current estimate of FPS through the `tracker.fps`\n    property.\n\n    Args:\n        maxlen: number of time differences to use for FPS calculation. Defaults to 30.\n    \"\"\"\n\n    def __init__(self, maxlen=30):\n        self.last_update = time.perf_counter()\n        self.dts = collections.deque(maxlen=maxlen)\n\n    def tick(self):\n        \"\"\"Update tracker (call this once per loop iteration).\"\"\"\n        now = time.perf_counter()\n        self.dts.append(now - self.last_update)\n        self.last_update = now\n\n    @property\n    def fps(self) -&gt; float:\n        \"\"\"Frames per second calculated from average time difference between updates.\"\"\"\n        if len(self.dts) &gt; 0:\n            return 1 / (sum(self.dts) / len(self.dts))\n        return 1\n</code></pre>"},{"location":"reference/helpers/#yarppg.helpers.FpsTracker.fps","title":"<code>fps: float</code>  <code>property</code>","text":"<p>Frames per second calculated from average time difference between updates.</p>"},{"location":"reference/helpers/#yarppg.helpers.FpsTracker.tick","title":"<code>tick()</code>","text":"<p>Update tracker (call this once per loop iteration).</p> Source code in <code>src\\yarppg\\helpers.py</code> <pre><code>def tick(self):\n    \"\"\"Update tracker (call this once per loop iteration).\"\"\"\n    now = time.perf_counter()\n    self.dts.append(now - self.last_update)\n    self.last_update = now\n</code></pre>"},{"location":"reference/helpers/#yarppg.helpers.bpm_from_frames_per_beat","title":"<code>bpm_from_frames_per_beat(hr, fps)</code>","text":"<p>Convert frames per beat to beats per minute (60 * fps / hr).</p> Source code in <code>src\\yarppg\\helpers.py</code> <pre><code>def bpm_from_frames_per_beat(hr: ArrayLike, fps: float) -&gt; np.ndarray:\n    \"\"\"Convert frames per beat to beats per minute (60 * fps / hr).\"\"\"\n    return 60 * fps / np.asarray(hr)\n</code></pre>"},{"location":"reference/helpers/#yarppg.helpers.frames_from_video","title":"<code>frames_from_video(filename)</code>","text":"<p>Read and yield frames from a video file.</p> Source code in <code>src\\yarppg\\helpers.py</code> <pre><code>def frames_from_video(filename: str | pathlib.Path) -&gt; Iterator[np.ndarray]:\n    \"\"\"Read and yield frames from a video file.\"\"\"\n    cap = cv2.VideoCapture(str(filename))\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        yield frame\n</code></pre>"},{"location":"reference/helpers/#yarppg.helpers.get_cached_resource_path","title":"<code>get_cached_resource_path(filename, url, reload=False)</code>","text":"<p>Download a file from the web and store it locally.</p> Source code in <code>src\\yarppg\\helpers.py</code> <pre><code>def get_cached_resource_path(filename: str, url: str, reload: bool = False):\n    \"\"\"Download a file from the web and store it locally.\"\"\"\n    RESOURCE_DIR.mkdir(exist_ok=True)\n    local_file = RESOURCE_DIR / filename\n    if not local_file.exists() or reload:\n        urllib.request.urlretrieve(url, filename=str(local_file))\n        if not local_file.exists():\n            raise FileNotFoundError(\n                f\"Something went wrong when getting {filename=:!r} from {url=:!r}.\"\n            )\n    return local_file\n</code></pre>"},{"location":"reference/helpers/#yarppg.helpers.get_video_fps","title":"<code>get_video_fps(filename)</code>","text":"<p>Find the frame rate of the given video file.</p> Source code in <code>src\\yarppg\\helpers.py</code> <pre><code>def get_video_fps(filename: str | pathlib.Path) -&gt; float:\n    \"\"\"Find the frame rate of the given video file.\"\"\"\n    if not pathlib.Path(filename).exists():\n        raise FileNotFoundError(f\"{filename=!r} not found.\")\n    cap = cv2.VideoCapture(str(filename))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    cap.release()\n    return fps\n</code></pre>"},{"location":"reference/hr_calculator/","title":"Heart rate estimation","text":"<p>Heart rate calculation utilities.</p>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.HrCalculator","title":"<code>HrCalculator</code>","text":"<p>Base class for heart rate calculation.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>class HrCalculator:\n    \"\"\"Base class for heart rate calculation.\"\"\"\n\n    def update(self, value: float) -&gt; float:  # noqa: ARG002\n        \"\"\"Process the new data and update HR estimate.\"\"\"\n        return np.nan\n\n    def reset(self) -&gt; None:\n        \"\"\"Clear the the internal state.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.HrCalculator.reset","title":"<code>reset()</code>","text":"<p>Clear the the internal state.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear the the internal state.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.HrCalculator.update","title":"<code>update(value)</code>","text":"<p>Process the new data and update HR estimate.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>def update(self, value: float) -&gt; float:  # noqa: ARG002\n    \"\"\"Process the new data and update HR estimate.\"\"\"\n    return np.nan\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.PeakBasedHrCalculator","title":"<code>PeakBasedHrCalculator</code>","text":"<p>               Bases: <code>HrCalculator</code></p> <p>Peak-based heart rate calculation.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>class PeakBasedHrCalculator(HrCalculator):\n    \"\"\"Peak-based heart rate calculation.\"\"\"\n\n    def __init__(\n        self,\n        fs: float,\n        window_seconds: float = 10,\n        distance: float = 0.5,\n        update_interval: int = 10,\n    ):\n        self.winsize = int(fs * window_seconds)\n        self.values = deque(maxlen=self.winsize)\n        self.mindist = int(fs * distance)\n\n        self.update_interval = update_interval\n        self.frames_seen = 0\n        self.last_hr = np.nan\n\n    def update(self, value: float) -&gt; float:\n        \"\"\"Process the new data and update HR estimate in frames per beat.\"\"\"\n        self.frames_seen += 1\n        self.values.append(value)\n        if (\n            len(self.values) &lt; self.winsize\n            or self.frames_seen % self.update_interval != 0\n        ):\n            return self.last_hr\n        peaks, _ = scipy.signal.find_peaks(self.values, distance=self.mindist)\n        self.last_hr = np.diff(peaks).mean()\n        return self.last_hr\n\n    def reset(self) -&gt; None:\n        \"\"\"Clear the internal buffer and intermediate values.\"\"\"\n        self.frames_seen = 0\n        self.values.clear()\n        self.last_hr = np.nan\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.PeakBasedHrCalculator.reset","title":"<code>reset()</code>","text":"<p>Clear the internal buffer and intermediate values.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear the internal buffer and intermediate values.\"\"\"\n    self.frames_seen = 0\n    self.values.clear()\n    self.last_hr = np.nan\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.PeakBasedHrCalculator.update","title":"<code>update(value)</code>","text":"<p>Process the new data and update HR estimate in frames per beat.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>def update(self, value: float) -&gt; float:\n    \"\"\"Process the new data and update HR estimate in frames per beat.\"\"\"\n    self.frames_seen += 1\n    self.values.append(value)\n    if (\n        len(self.values) &lt; self.winsize\n        or self.frames_seen % self.update_interval != 0\n    ):\n        return self.last_hr\n    peaks, _ = scipy.signal.find_peaks(self.values, distance=self.mindist)\n    self.last_hr = np.diff(peaks).mean()\n    return self.last_hr\n</code></pre>"},{"location":"reference/rppg/","title":"rPPG orchestrator","text":"<p>Provides the Rppg orchestrator class.</p> <p>The orchestrator ties together the typical steps required in an rPPG pipeline:</p> <ol> <li>region of interest (ROI) identification (yarppg.roi)</li> <li>rPPG signal extraction (yarppg.processors)</li> <li>heart rate estimation (yarppg.hr_calculator)</li> </ol> <p><code>Rppg</code>'s <code>process_frame</code> method performs the three steps from above in order and produces an yarppg.containers.RppgResult that holds the extracted rPPG signal value as well as the frame, ROI and some additional information.</p> <pre><code>import yarppg\n\ndefault_settings = yarppg.Settings()\nrppg = yarppg.Rppg.from_settings(default_settings)\n\nresult = rppg.process_frame(frame)  # input a (h x w x 3)-image array.\nprint(result.hr)\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg","title":"<code>Rppg</code>","text":"<p>Orchestrator for the complete rPPG pipeline.</p> <p>If unspecified the following default configuration is used:</p> <ul> <li><code>FaceMeshDetector</code> is used for ROI identification.</li> <li>The base <code>Processor</code> extracts the average green value.</li> <li>A <code>PeakBasedHrCalculator</code> estimates HR</li> </ul> <p>Parameters:</p> Name Type Description Default <code>roi_detector</code> <code>RoiDetector | None</code> <p>detector for identifying the region of interest (and background).</p> <code>None</code> <code>processor</code> <code>Processor | None</code> <p>rPPG signal extraction algorithm.</p> <code>None</code> <code>hr_calc</code> <code>HrCalculator | None</code> <p>heart rate calculation algorithm.</p> <code>None</code> <code>fps</code> <code>float</code> <p>expected frames per second of the camera/video</p> <code>30</code> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>class Rppg:\n    \"\"\"Orchestrator for the complete rPPG pipeline.\n\n    If unspecified the following default configuration is used:\n\n    - [`FaceMeshDetector`][yarppg.FaceMeshDetector] is used for ROI identification.\n    - The base [`Processor`][yarppg.Processor] extracts the average green value.\n    - A [`PeakBasedHrCalculator`][yarppg.PeakBasedHrCalculator] estimates HR\n\n    Args:\n       roi_detector: detector for identifying the region of interest (and background).\n       processor: rPPG signal extraction algorithm.\n       hr_calc: heart rate calculation algorithm.\n       fps: expected frames per second of the camera/video\n    \"\"\"\n\n    def __init__(\n        self,\n        roi_detector: roi.RoiDetector | None = None,\n        processor: processors.Processor | None = None,\n        hr_calc: hr_calculator.HrCalculator | None = None,\n        fps: float = 30,\n    ):\n        self.roi_detector = roi_detector or roi.FaceMeshDetector()\n        self.processor = processor or processors.Processor()\n        self.hr_calculator = hr_calc or hr_calculator.PeakBasedHrCalculator(fps)\n\n    def process_frame(self, frame: np.ndarray) -&gt; RppgResult:\n        \"\"\"Process a single frame from video or live stream.\"\"\"\n        roi = self.roi_detector.detect(frame)\n        result = self.processor.process(roi)\n        result.hr = self.hr_calculator.update(result.value)\n\n        return result\n\n    @overload\n    def process_video(self, filename: ..., as_dataframe: Literal[True]) -&gt; pd.DataFrame:\n        ...\n\n    @overload\n    def process_video(\n        self, filename: ..., as_dataframe: Literal[False] = ...\n    ) -&gt; list[RppgResult]:\n        ...\n\n    def process_video(self, filename: str | pathlib.Path, as_dataframe=False):\n        \"\"\"Convenience function to process an entire video file at once.\"\"\"\n        results = []\n        for frame in helpers.frames_from_video(filename):\n            result = self.process_frame(frame)\n            if as_dataframe:\n                results.append(result.to_series())\n            else:\n                results.append(result)\n        if as_dataframe:\n            return pd.concat(results).T\n        return results\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset internal elements.\"\"\"\n        self.processor.reset()\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -&gt; \"Rppg\":\n        \"\"\"Instantiate rPPG orchestrator with the given settings.\"\"\"\n        detector = roi.detectors[settings.detector]()\n        processor = processors.algorithms[settings.algorithm]()\n        if settings.filter:\n            if settings.filter == \"bandpass\":\n                b, a = scipy.signal.iirfilter(2, [0.7, 1.8], fs=30, btype=\"band\")\n                livefilter = digital_filter.DigitalFilter(b, a)\n            else:\n                livefilter = digital_filter.make_digital_filter(settings.filter)\n            processor = processors.FilteredProcessor(processor, livefilter)\n        return cls(detector, processor)\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.from_settings","title":"<code>from_settings(settings)</code>  <code>classmethod</code>","text":"<p>Instantiate rPPG orchestrator with the given settings.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>@classmethod\ndef from_settings(cls, settings: Settings) -&gt; \"Rppg\":\n    \"\"\"Instantiate rPPG orchestrator with the given settings.\"\"\"\n    detector = roi.detectors[settings.detector]()\n    processor = processors.algorithms[settings.algorithm]()\n    if settings.filter:\n        if settings.filter == \"bandpass\":\n            b, a = scipy.signal.iirfilter(2, [0.7, 1.8], fs=30, btype=\"band\")\n            livefilter = digital_filter.DigitalFilter(b, a)\n        else:\n            livefilter = digital_filter.make_digital_filter(settings.filter)\n        processor = processors.FilteredProcessor(processor, livefilter)\n    return cls(detector, processor)\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.process_frame","title":"<code>process_frame(frame)</code>","text":"<p>Process a single frame from video or live stream.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>def process_frame(self, frame: np.ndarray) -&gt; RppgResult:\n    \"\"\"Process a single frame from video or live stream.\"\"\"\n    roi = self.roi_detector.detect(frame)\n    result = self.processor.process(roi)\n    result.hr = self.hr_calculator.update(result.value)\n\n    return result\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.process_video","title":"<code>process_video(filename, as_dataframe=False)</code>","text":"<p>Convenience function to process an entire video file at once.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>def process_video(self, filename: str | pathlib.Path, as_dataframe=False):\n    \"\"\"Convenience function to process an entire video file at once.\"\"\"\n    results = []\n    for frame in helpers.frames_from_video(filename):\n        result = self.process_frame(frame)\n        if as_dataframe:\n            results.append(result.to_series())\n        else:\n            results.append(result)\n    if as_dataframe:\n        return pd.concat(results).T\n    return results\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.reset","title":"<code>reset()</code>","text":"<p>Reset internal elements.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset internal elements.\"\"\"\n    self.processor.reset()\n</code></pre>"},{"location":"reference/settings/","title":"Settings","text":"<p>Provides configuration containers for the yarPPG application.</p>"},{"location":"reference/settings/#yarppg.settings.HydraSettings","title":"<code>HydraSettings</code>  <code>dataclass</code>","text":"<p>Base class for Hydra-based configurations.</p> <p>Mainly manages the hydra-specific settings, deactivating its directory and output management, so that running with hydra.main does not behave differently from a normal CLI.</p> Source code in <code>src\\yarppg\\settings.py</code> <pre><code>@dataclasses.dataclass(kw_only=True)\nclass HydraSettings:\n    \"\"\"Base class for Hydra-based configurations.\n\n    Mainly manages the hydra-specific settings, deactivating its directory and output\n    management, so that running with hydra.main does not behave differently from a\n    normal CLI.\n    \"\"\"\n\n    hydra: \"hydra.conf.HydraConf\" = dataclasses.field(\n        default_factory=lambda: hydra.conf.HydraConf(\n            output_subdir=None,\n            run=hydra.conf.RunDir(\".\"),\n            help=hydra.conf.HelpConf(app_name=\"run-yarppg\"),\n            overrides=hydra.conf.OverridesConf(\n                # hydra=[\"job_logging=null\", \"hydra_logging=null\"]\n            ),\n        )\n    )\n</code></pre>"},{"location":"reference/settings/#yarppg.settings.Settings","title":"<code>Settings</code>  <code>dataclass</code>","text":"<p>               Bases: <code>HydraSettings</code></p> <p>Comprises all configuration options available in the yarppg application.</p> Source code in <code>src\\yarppg\\settings.py</code> <pre><code>@dataclasses.dataclass\nclass Settings(HydraSettings):\n    \"\"\"Comprises all configuration options available in the yarppg application.\"\"\"\n\n    ui: Any\n    savepath: str | None = None\n    detector: str = \"facemesh\"\n    filter: FilterConfig | None = dataclasses.field(\n        default_factory=lambda: FilterConfig(30, 0.5, 2, btype=\"bandpass\")\n    )\n    algorithm: str = \"green\"\n    defaults: Any = dataclasses.field(\n        default_factory=lambda: [\n            {\"ui\": \"simplest\"},\n            \"_self_\",\n        ]\n    )\n</code></pre>"},{"location":"reference/settings/#yarppg.settings.UiSettings","title":"<code>UiSettings</code>  <code>dataclass</code>","text":"<p>Settings for the user interface.</p> Source code in <code>src\\yarppg\\settings.py</code> <pre><code>@dataclasses.dataclass\nclass UiSettings:\n    \"\"\"Settings for the user interface.\"\"\"\n</code></pre>"},{"location":"reference/settings/#yarppg.settings.available_ui_configs","title":"<code>available_ui_configs()</code>","text":"<p>Check availability of UIs and return each corresponding settings container.</p> Source code in <code>src\\yarppg\\settings.py</code> <pre><code>def available_ui_configs():\n    \"\"\"Check availability of UIs and return each corresponding settings container.\"\"\"\n    import yarppg.ui.simplest\n\n    uis: dict[str, Any] = {\"simplest\": yarppg.ui.simplest.SimplestOpenCvWindowSettings}\n    try:\n        import yarppg.ui.qt6\n\n        uis[\"qt6_simple\"] = yarppg.ui.qt6.SimpleQt6WindowSettings\n    except (ModuleNotFoundError, ImportError):\n        pass\n\n    return uis\n</code></pre>"},{"location":"reference/settings/#yarppg.settings.get_config","title":"<code>get_config(argv)</code>","text":"<p>Get the default configuration with optional overrides.</p> Source code in <code>src\\yarppg\\settings.py</code> <pre><code>def get_config(argv: list[str] | None) -&gt; Settings:\n    \"\"\"Get the default configuration with optional overrides.\"\"\"\n    register_schemas()\n    with hydra.initialize():\n        cfg = hydra.compose(config_name=\"config\", overrides=argv)\n    return hydra.utils.instantiate(cfg)\n</code></pre>"},{"location":"reference/settings/#yarppg.settings.register_schemas","title":"<code>register_schemas()</code>","text":"<p>Register base schema and settings for available UI implementations.</p> Source code in <code>src\\yarppg\\settings.py</code> <pre><code>def register_schemas():\n    \"\"\"Register base schema and settings for available UI implementations.\"\"\"\n    cs = hydra.core.config_store.ConfigStore.instance()\n    cs.store(name=\"config\", node=Settings)\n    for name, cfg_class in available_ui_configs().items():\n        cs.store(name=name, node=cfg_class, group=\"ui\")\n</code></pre>"},{"location":"reference/processors/","title":"rPPG processors","text":"<p>Implementations of various rPPG signal extractors found in literature.</p> <p>All processors base the <code>Processor</code> which most importantly features a <code>process</code> method. The <code>process</code> function takes an <code>RegionOfInterest</code> container and extracts the rPPG signal value.</p> <p>Note that this is a stateful function, for most processors. Many algorithms use an internal buffer of previous values to provide a more robust calculation. To clear the internal buffer, we can call <code>reset</code></p> <p>Processors can be wrapped in a <code>FilteredProcessor</code> allowing for ad-hoc signal smoothing with each signal update.</p> <p>Besides the base processor, the following additional algorithms from literature are implemented:</p>"},{"location":"reference/processors/#yarppg.processors--chromprocessor-experimental","title":"ChromProcessor (experimental)","text":"<p>Implements the chrominance-based algorithm by de Haan, &amp; Jeanne (2013).</p>"},{"location":"reference/processors/#yarppg.processors--more-to-come-your-contributions-are-welcome","title":"More to come (your contributions are welcome)","text":""},{"location":"reference/processors/chrom/","title":"ChromProcessor","text":"<p>Chrominance-based rPPG method introduced by de Haan &amp; Jeanne (2013).</p> <p>de Haan, G., &amp; Jeanne, V. (2013). Robust Pulse Rate From Chrominance-Based rPPG. IEEE Transactions on Biomedical Engineering, 60(10), 2878-2886. https://doi.org/10.1109/TBME.2013.2266196</p>"},{"location":"reference/processors/chrom/#yarppg.processors.chrom.ChromProcessor","title":"<code>ChromProcessor</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Chrominance-based rPPG algorithm by de Haan &amp; Jeanne (2013).</p> <p>Parameters:</p> Name Type Description Default <code>winsize</code> <code>int</code> <p>window size for moving average calculations. Defaults to 45.</p> <code>45</code> <code>method</code> <code>Literal['fixed', 'xovery']</code> <p>method to use. Can be 'xovery' or 'fixed'. Defaults to \"xovery\".</p> <code>'xovery'</code> Source code in <code>src\\yarppg\\processors\\chrom.py</code> <pre><code>class ChromProcessor(Processor):\n    \"\"\"Chrominance-based rPPG algorithm by de Haan &amp; Jeanne (2013).\n\n    Args:\n        winsize: window size for moving average calculations. Defaults to 45.\n        method: method to use. Can be 'xovery' or 'fixed'. Defaults to \"xovery\".\n    \"\"\"\n\n    def __init__(\n        self, winsize: int = 45, method: Literal[\"fixed\", \"xovery\"] = \"xovery\"\n    ):\n        Processor.__init__(self)\n\n        self.winsize = winsize\n        self.method = method\n\n        self._rgbs: list[Color] = []\n        self._xs: list[float] = []\n        self._ys: list[float] = []\n\n    def process(self, roi: RegionOfInterest) -&gt; RppgResult:\n        \"\"\"Calculate pulse signal update according to Chrom algorithm.\"\"\"\n        result = super().process(roi)\n        self._rgbs.append(result.roi_mean)\n\n        if self.method == \"fixed\":\n            result.value = self._calculate_fixed_update()\n\n        elif self.method == \"xovery\":\n            result.value = self._calculate_xovery_update()\n\n        return result\n\n    def _calculate_fixed_update(self) -&gt; float:\n        rgbmean = Color.from_array(np.mean(self._rgbs[-self.winsize :], axis=0))\n\n        rn = self._rgbs[-1].r / (rgbmean.r or 1.0)\n        gn = self._rgbs[-1].g / (rgbmean.g or 1.0)\n        bn = self._rgbs[-1].b / (rgbmean.b or 1.0)\n\n        self._xs.append(3 * rn - 2 * gn)\n        self._ys.append(1.5 * rn + gn - 1.5 * bn)\n\n        return self._xs[-1] / (self._ys[-1] or 1.0) - 1\n\n    def _calculate_xovery_update(self) -&gt; float:\n        rgb = self._rgbs[-1]\n\n        self._xs.append(rgb.r - rgb.g)\n        self._ys.append(0.5 * rgb.r + 0.5 * rgb.g - rgb.b)\n\n        xmean = np.mean(self._xs[-self.winsize :])\n        ymean = np.mean(self._ys[-self.winsize :])\n\n        return float(xmean / (ymean or 1) - 1)\n\n    def reset(self):\n        \"\"\"Reset internal state and intermediate values.\"\"\"\n        self._rgbs.clear()\n        self._xs.clear()\n        self._ys.clear()\n</code></pre>"},{"location":"reference/processors/chrom/#yarppg.processors.chrom.ChromProcessor.process","title":"<code>process(roi)</code>","text":"<p>Calculate pulse signal update according to Chrom algorithm.</p> Source code in <code>src\\yarppg\\processors\\chrom.py</code> <pre><code>def process(self, roi: RegionOfInterest) -&gt; RppgResult:\n    \"\"\"Calculate pulse signal update according to Chrom algorithm.\"\"\"\n    result = super().process(roi)\n    self._rgbs.append(result.roi_mean)\n\n    if self.method == \"fixed\":\n        result.value = self._calculate_fixed_update()\n\n    elif self.method == \"xovery\":\n        result.value = self._calculate_xovery_update()\n\n    return result\n</code></pre>"},{"location":"reference/processors/chrom/#yarppg.processors.chrom.ChromProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset internal state and intermediate values.</p> Source code in <code>src\\yarppg\\processors\\chrom.py</code> <pre><code>def reset(self):\n    \"\"\"Reset internal state and intermediate values.\"\"\"\n    self._rgbs.clear()\n    self._xs.clear()\n    self._ys.clear()\n</code></pre>"},{"location":"reference/processors/processor/","title":"Basic processor","text":"<p>Provides base classes for rPPG signal computation.</p>"},{"location":"reference/processors/processor/#yarppg.processors.processor.Processor","title":"<code>Processor</code>","text":"<p>Base rPPG processor, extracting the average green channel from the ROI.</p> Source code in <code>src\\yarppg\\processors\\processor.py</code> <pre><code>class Processor:\n    \"\"\"Base rPPG processor, extracting the average green channel from the ROI.\"\"\"\n\n    def process(self, roi: RegionOfInterest) -&gt; RppgResult:\n        \"\"\"Calculate average green channel in the roi area.\"\"\"\n        avg = masked_average(roi.baseimg, roi.mask)\n        bg_mean = Color.null()\n        if roi.bg_mask is not None:\n            bg_mean = masked_average(roi.baseimg, roi.bg_mask)\n\n        return RppgResult(avg.g, roi, roi_mean=avg, bg_mean=bg_mean)\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset internal state and intermediate values.\"\"\"\n        pass  # no persistent values in base class\n</code></pre>"},{"location":"reference/processors/processor/#yarppg.processors.processor.Processor.process","title":"<code>process(roi)</code>","text":"<p>Calculate average green channel in the roi area.</p> Source code in <code>src\\yarppg\\processors\\processor.py</code> <pre><code>def process(self, roi: RegionOfInterest) -&gt; RppgResult:\n    \"\"\"Calculate average green channel in the roi area.\"\"\"\n    avg = masked_average(roi.baseimg, roi.mask)\n    bg_mean = Color.null()\n    if roi.bg_mask is not None:\n        bg_mean = masked_average(roi.baseimg, roi.bg_mask)\n\n    return RppgResult(avg.g, roi, roi_mean=avg, bg_mean=bg_mean)\n</code></pre>"},{"location":"reference/processors/processor/#yarppg.processors.processor.Processor.reset","title":"<code>reset()</code>","text":"<p>Reset internal state and intermediate values.</p> Source code in <code>src\\yarppg\\processors\\processor.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset internal state and intermediate values.\"\"\"\n    pass  # no persistent values in base class\n</code></pre>"},{"location":"reference/processors/processor/#yarppg.processors.processor.FilteredProcessor","title":"<code>FilteredProcessor</code>","text":"<p>               Bases: <code>Processor</code></p> <p>Processor with temporal filtering of the extracted signal.</p> Source code in <code>src\\yarppg\\processors\\processor.py</code> <pre><code>class FilteredProcessor(Processor):\n    \"\"\"Processor with temporal filtering of the extracted signal.\"\"\"\n\n    def __init__(self, processor: Processor, livefilter: DigitalFilter | None = None):\n        self.processor = processor\n        self.livefilter = livefilter\n\n    def process(self, roi: RegionOfInterest) -&gt; RppgResult:\n        \"\"\"Calculate processor output and apply digital filter.\"\"\"\n        result = self.processor.process(roi)\n        if self.livefilter is not None and np.isfinite(result.value):\n            # only calculate filter update if not NaN\n            result.value = self.livefilter.process(result.value)\n        return result\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset internal state and intermediate values.\"\"\"\n        self.processor.reset()\n        if self.livefilter is not None:\n            self.livefilter.reset()\n</code></pre>"},{"location":"reference/processors/processor/#yarppg.processors.processor.FilteredProcessor.process","title":"<code>process(roi)</code>","text":"<p>Calculate processor output and apply digital filter.</p> Source code in <code>src\\yarppg\\processors\\processor.py</code> <pre><code>def process(self, roi: RegionOfInterest) -&gt; RppgResult:\n    \"\"\"Calculate processor output and apply digital filter.\"\"\"\n    result = self.processor.process(roi)\n    if self.livefilter is not None and np.isfinite(result.value):\n        # only calculate filter update if not NaN\n        result.value = self.livefilter.process(result.value)\n    return result\n</code></pre>"},{"location":"reference/processors/processor/#yarppg.processors.processor.FilteredProcessor.reset","title":"<code>reset()</code>","text":"<p>Reset internal state and intermediate values.</p> Source code in <code>src\\yarppg\\processors\\processor.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset internal state and intermediate values.\"\"\"\n    self.processor.reset()\n    if self.livefilter is not None:\n        self.livefilter.reset()\n</code></pre>"},{"location":"reference/roi/","title":"ROI detectors","text":"<p>Utilities for ROI (region of interest) detection and manipulation.</p> <p>yarPPG comes with a number of ROI detectors, which find the face in the input frame and provide a mask with the relevant region(s). The following detectors are currently implemented:</p> <ul> <li><code>FaceMeshDetector</code> (default) - uses MediaPipe's   FaceMesh landmarker.</li> <li><code>SelfieDetector</code> - uses MediaPipe's SelfieSegmenter   solution. Selfie segmentation is slower than FaceMesh and may not work in a   real-time application.</li> </ul> <p>Detectors return a <code>RegionOfInterest</code> container that stores the original image, the ROI mask and an optional background mask.</p>"},{"location":"reference/roi/facemesh_detector/","title":"MediaPipe FaceMesh","text":"<p>Detect the lower face with MediaPipe's FaceMesh detector.</p> <p>This detector is based on the face landmarker task from MediaPipe. The face landmarker provides locations of more than 450 facial landmarks. From these, we can define a region for the lower face, as is done for example by Li et al. (2014)<sup>1</sup>.</p> <ol> <li> <p>X. Li, J. Chen, G. Zhao, and M. Pietikainen, \u201cRemote Heart Rate Measurement From Face Videos Under Realistic Situations\u201d, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4264-4271, 2014 doi:10.1109/CVPR.2014.543 \u21a9</p> </li> </ol>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.FaceMeshDetector","title":"<code>FaceMeshDetector</code>","text":"<p>               Bases: <code>RoiDetector</code></p> <p>Face detector using MediaPipe's face landmarker.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>class FaceMeshDetector(RoiDetector):\n    \"\"\"Face detector using MediaPipe's face landmarker.\"\"\"\n\n    _lower_face = [200, 431, 411, 340, 349, 120, 111, 187, 211]\n\n    def __init__(self, draw_landmarks=False, **kwargs):\n        super().__init__(**kwargs)\n        modelpath = get_face_landmarker_modelfile()\n        if modelpath is None:\n            raise FileNotFoundError(\"Could not find or download landmarker model file.\")\n        base_options = mp.tasks.BaseOptions(model_asset_path=modelpath)\n        landmarker_options = mp.tasks.vision.FaceLandmarkerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO,\n        )\n        self.landmarker = mp.tasks.vision.FaceLandmarker.create_from_options(\n            landmarker_options\n        )\n        self.draw_landmarks = draw_landmarks\n\n    def __del__(self):\n        self.landmarker.close()\n\n    def _process_landmarks(self, frame, results) -&gt; tuple[np.ndarray, np.ndarray]:\n        height, width = frame.shape[:2]\n        coords = get_landmark_coords(results.face_landmarks[0], width, height)[:, :2]\n        face_rect = get_boundingbox_from_coords(coords)\n\n        mask = contour_to_mask((height, width), coords[self._lower_face])\n        return mask, face_rect\n\n    def detect(self, frame: np.ndarray) -&gt; RegionOfInterest:\n        \"\"\"Find face landmarks and create ROI around the lower face region.\"\"\"\n        rawimg = frame.copy()\n        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            results = self.landmarker.detect_for_video(\n                mp_image, int(time.perf_counter() * 1000)\n            )\n\n        if len(results.face_landmarks) &lt; 1:\n            return RegionOfInterest(np.zeros_like(frame), baseimg=frame)\n\n        if self.draw_landmarks:\n            self.draw_facemesh(frame, results.face_landmarks[0], tesselate=True)\n\n        mask, face_rect = self._process_landmarks(frame, results)\n        return RegionOfInterest(mask, baseimg=rawimg, face_rect=tuple(face_rect))\n\n    def draw_facemesh(\n        self,\n        img,\n        face_landmarks,\n        tesselate=False,\n        contour=False,\n        irises=False,\n    ):\n        \"\"\"Draw the detected face landmarks on the image.\"\"\"\n        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()  # type: ignore\n        face_landmarks_proto.landmark.extend(\n            [\n                landmark_pb2.NormalizedLandmark(  # type: ignore\n                    x=landmark.x, y=landmark.y, z=landmark.z\n                )\n                for landmark in face_landmarks\n            ]\n        )\n        if tesselate:\n            mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n                image=img,\n                landmark_list=face_landmarks_proto,\n                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,  # type: ignore\n                landmark_drawing_spec=None,\n                connection_drawing_spec=TESSELATION_SPEC,\n            )\n        if contour:\n            mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n                image=img,\n                landmark_list=face_landmarks_proto,\n                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,  # type: ignore\n                landmark_drawing_spec=None,\n                connection_drawing_spec=CONTOUR_SPEC,\n            )\n        if irises:\n            mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n                image=img,\n                landmark_list=face_landmarks_proto,\n                connections=mp.solutions.face_mesh.FACEMESH_IRISES,  # type: ignore\n                landmark_drawing_spec=None,\n                connection_drawing_spec=IRISES_SPEC,\n            )\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.FaceMeshDetector.detect","title":"<code>detect(frame)</code>","text":"<p>Find face landmarks and create ROI around the lower face region.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def detect(self, frame: np.ndarray) -&gt; RegionOfInterest:\n    \"\"\"Find face landmarks and create ROI around the lower face region.\"\"\"\n    rawimg = frame.copy()\n    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        results = self.landmarker.detect_for_video(\n            mp_image, int(time.perf_counter() * 1000)\n        )\n\n    if len(results.face_landmarks) &lt; 1:\n        return RegionOfInterest(np.zeros_like(frame), baseimg=frame)\n\n    if self.draw_landmarks:\n        self.draw_facemesh(frame, results.face_landmarks[0], tesselate=True)\n\n    mask, face_rect = self._process_landmarks(frame, results)\n    return RegionOfInterest(mask, baseimg=rawimg, face_rect=tuple(face_rect))\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.FaceMeshDetector.draw_facemesh","title":"<code>draw_facemesh(img, face_landmarks, tesselate=False, contour=False, irises=False)</code>","text":"<p>Draw the detected face landmarks on the image.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def draw_facemesh(\n    self,\n    img,\n    face_landmarks,\n    tesselate=False,\n    contour=False,\n    irises=False,\n):\n    \"\"\"Draw the detected face landmarks on the image.\"\"\"\n    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()  # type: ignore\n    face_landmarks_proto.landmark.extend(\n        [\n            landmark_pb2.NormalizedLandmark(  # type: ignore\n                x=landmark.x, y=landmark.y, z=landmark.z\n            )\n            for landmark in face_landmarks\n        ]\n    )\n    if tesselate:\n        mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n            image=img,\n            landmark_list=face_landmarks_proto,\n            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,  # type: ignore\n            landmark_drawing_spec=None,\n            connection_drawing_spec=TESSELATION_SPEC,\n        )\n    if contour:\n        mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n            image=img,\n            landmark_list=face_landmarks_proto,\n            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,  # type: ignore\n            landmark_drawing_spec=None,\n            connection_drawing_spec=CONTOUR_SPEC,\n        )\n    if irises:\n        mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n            image=img,\n            landmark_list=face_landmarks_proto,\n            connections=mp.solutions.face_mesh.FACEMESH_IRISES,  # type: ignore\n            landmark_drawing_spec=None,\n            connection_drawing_spec=IRISES_SPEC,\n        )\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.get_boundingbox_from_coords","title":"<code>get_boundingbox_from_coords(coords)</code>","text":"<p>Calculate the bounding rectangle containing all landmarks.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def get_boundingbox_from_coords(coords: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the bounding rectangle containing all landmarks.\"\"\"\n    xy = np.min(coords, axis=0)\n    wh = np.subtract(np.max(coords, axis=0), xy)\n\n    return np.r_[xy, wh]\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.get_face_landmarker_modelfile","title":"<code>get_face_landmarker_modelfile()</code>","text":"<p>Get the filename of the FaceLandmarker - download file if necessary.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def get_face_landmarker_modelfile():\n    \"\"\"Get the filename of the FaceLandmarker - download file if necessary.\"\"\"\n    task_filename = \"face_landmarker.task\"\n    return get_cached_resource_path(\n        task_filename, MEDIAPIPE_MODELS_BASE + LANDMARKER_TASK\n    )\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.get_landmark_coords","title":"<code>get_landmark_coords(landmarks, width, height)</code>","text":"<p>Extract normalized landmark coordinates to array of pixel coordinates.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def get_landmark_coords(\n    landmarks: list[landmark_module.NormalizedLandmark], width: int, height: int\n) -&gt; np.ndarray:\n    \"\"\"Extract normalized landmark coordinates to array of pixel coordinates.\"\"\"\n    xyz = [(lm.x, lm.y, lm.z) for lm in landmarks]\n    return np.multiply(xyz, [width, height, width]).astype(int)\n</code></pre>"},{"location":"reference/roi/selfie_detector/","title":"MediaPipe Selfie Segmenter","text":"<p>Detect the face skin region with MediaPipe's selfie segmentation.</p> <p>This method is very slow (150-200ms per frame) and will not properly work in a real-time setting. <code>FaceMeshDetector</code> should be used instead.</p> <p>More information on the selfie segmenter can be found here: https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter#multiclass-model</p>"},{"location":"reference/roi/selfie_detector/#yarppg.roi.selfie_segmenter.SelfieDetector","title":"<code>SelfieDetector</code>","text":"<p>               Bases: <code>RoiDetector</code></p> <p>Face detector based on MediaPipe's selfie segmentation task.</p> Source code in <code>src\\yarppg\\roi\\selfie_segmenter.py</code> <pre><code>class SelfieDetector(RoiDetector):\n    \"\"\"Face detector based on MediaPipe's selfie segmentation task.\"\"\"\n\n    def __init__(self, confidence=0.5, **kwargs):\n        super().__init__(**kwargs)\n        self.confidence = confidence\n\n        modelpath = get_selfie_segmenter_modelfile()\n        if modelpath is None:\n            raise FileNotFoundError(\"Could not find or download segmenter model file.\")\n\n        base_options = mp.tasks.BaseOptions(model_asset_path=modelpath)\n        segmenter_options = mp.tasks.vision.ImageSegmenterOptions(\n            base_options=base_options, running_mode=mp.tasks.vision.RunningMode.VIDEO\n        )\n        self.segmenter = mp.tasks.vision.ImageSegmenter.create_from_options(\n            segmenter_options\n        )\n\n    def __del__(self):\n        self.segmenter.close()\n\n    def detect(self, frame: np.ndarray) -&gt; RegionOfInterest:\n        \"\"\"Identify face skin region and background in the given image.\"\"\"\n        rawimg = frame.copy()\n        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n\n        results = self.segmenter.segment_for_video(\n            mp_image, int(time.perf_counter() * 1000)\n        )\n\n        face_mask = results.confidence_masks[3].numpy_view() &gt; self.confidence\n        bg_mask = results.confidence_masks[0].numpy_view() &gt; self.confidence\n        return RegionOfInterest(\n            face_mask.astype(np.uint8), baseimg=rawimg, bg_mask=bg_mask.astype(np.uint8)\n        )\n</code></pre>"},{"location":"reference/roi/selfie_detector/#yarppg.roi.selfie_segmenter.SelfieDetector.detect","title":"<code>detect(frame)</code>","text":"<p>Identify face skin region and background in the given image.</p> Source code in <code>src\\yarppg\\roi\\selfie_segmenter.py</code> <pre><code>def detect(self, frame: np.ndarray) -&gt; RegionOfInterest:\n    \"\"\"Identify face skin region and background in the given image.\"\"\"\n    rawimg = frame.copy()\n    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n\n    results = self.segmenter.segment_for_video(\n        mp_image, int(time.perf_counter() * 1000)\n    )\n\n    face_mask = results.confidence_masks[3].numpy_view() &gt; self.confidence\n    bg_mask = results.confidence_masks[0].numpy_view() &gt; self.confidence\n    return RegionOfInterest(\n        face_mask.astype(np.uint8), baseimg=rawimg, bg_mask=bg_mask.astype(np.uint8)\n    )\n</code></pre>"},{"location":"reference/roi/selfie_detector/#yarppg.roi.selfie_segmenter.get_selfie_segmenter_modelfile","title":"<code>get_selfie_segmenter_modelfile()</code>","text":"<p>Get the filename of the SelfieSegmenter - download file if necessary.</p> Source code in <code>src\\yarppg\\roi\\selfie_segmenter.py</code> <pre><code>def get_selfie_segmenter_modelfile():\n    \"\"\"Get the filename of the SelfieSegmenter - download file if necessary.\"\"\"\n    task_filename = \"selfie_multiclass.tflite\"\n    return get_cached_resource_path(task_filename, MEDIAPIPE_MODELS_BASE + SELFIE_TASK)\n</code></pre>"},{"location":"reference/ui/","title":"User interfaces","text":"<p>Provides user interfaces for yarPPG.</p> <p>yarPPG comes with several user interfaces based on additional optional dependencies. Make sure to install the corresponding extras to use special UIs, instead of the OpenCV based <code>simplest</code> UI.</p>"},{"location":"reference/ui/#yarppg.ui--available-uis","title":"Available UIs","text":""},{"location":"reference/ui/#yarppg.ui--simplest","title":"Simplest","text":"<p>This is a simple infinite loop, grabbing new frames from the camera and visualizing the results in an OpenCV window.</p> <p>No additional depencies are required for the default interface.</p>"},{"location":"reference/ui/#yarppg.ui--simple-qt6-window","title":"Simple Qt6 window","text":"<p>A small GUI window highlighting the detected ROI and a trace of the extracted rPPG signal. Make sure to install extras with: <pre><code>pip install \".[qt6]\"\n</code></pre></p>"},{"location":"reference/ui/#yarppg.ui--more-to-come","title":"More to come","text":"<p>Feel free to contribute other user interfaces using any framework.</p>"},{"location":"reference/ui/#yarppg.ui.launch_ui","title":"<code>launch_ui(rppg, ui_settings)</code>","text":"<p>Launch a user interface for the given configuration.</p> Source code in <code>src\\yarppg\\ui\\__init__.py</code> <pre><code>def launch_ui(rppg: yarppg.Rppg, ui_settings: yarppg.UiSettings) -&gt; int:\n    \"\"\"Launch a user interface for the given configuration.\"\"\"\n    if type(ui_settings).__name__ == \"SimpleQt6WindowSettings\":\n        from yarppg.ui.qt6.simple_window import SimpleQt6WindowSettings, launch_window\n\n        assert isinstance(ui_settings, SimpleQt6WindowSettings)\n        return launch_window(rppg, ui_settings)\n\n    if type(ui_settings).__name__ == \"SimplestOpenCvWindowSettings\":\n        from yarppg.ui.simplest import SimplestOpenCvWindowSettings, launch_loop\n\n        assert isinstance(ui_settings, SimplestOpenCvWindowSettings)\n        return launch_loop(rppg, ui_settings)\n\n    raise NotImplementedError(f\"Cannot understand the given {ui_settings!r}\")\n</code></pre>"}]}