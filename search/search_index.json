{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the yarPPG documentation","text":"<p>yarPPG is yet another implementation of remote PhotoPlethysmoGraphy. Remote photo\u00adplethysmography (rPPG) refers to the camera-based measurement of a blood volume pulse signal. It works by detecting small changes in skin color, originating from the pulsation of blood<sup>1</sup>.</p> <p>[!CAUTION] This is just a hobby project. Intended for demo purposes only, the   provided program/code is not suitable to be used in a clinical setup   or for any decision making in general.</p>"},{"location":"#installation-and-usage","title":"Installation and usage","text":"<p>In order to run the yarPPG application, clone this repository and navigate to the downloaded folder. You can then install the folder into your Python environment. This will install the <code>run-yarppg</code> command.</p> <pre><code>git clone https://github.com/SamProell/yarppg.git\ncd yarppg\npip install \".\"\nrun-yarppg\n</code></pre>"},{"location":"#core-functionality","title":"Core functionality","text":"<p>Different from earlier versions of yarPPG, the core functionality for remote PPG signal extraction has been completely decoupled from the user interface. The <code>Rppg</code> class combines all required steps (roi identification, signal extraction, heart rate estimation) into one (stateful) function.</p> <pre><code>import yarppg\n\nrppg = yarppg.Rppg()\n\nwhile running:\n    # frame = ...  # get an image array of shape h x w x 3\n    result = rppg.process_frame(frame)\n    print(f\"Current rPPG signal value: {result.value} (HR: {result.hr})\")\n</code></pre> <p>See [this guide] if you need more fine-grained control over the individual calculation steps.</p> <p>The <code>Rppg</code> class also comes with a method to process an entire video file at once. See more details [here].</p>"},{"location":"#user-interfaces","title":"User interfaces","text":"<p>The default user interface launched by the <code>run-yarppg</code> command is a simplistic window based on OpenCV. More elaborate user interfaces are available, but require additional dependencies.</p>"},{"location":"#simple-qt6-window","title":"Simple Qt6 window","text":"<pre><code>pip install \".[qt6]\"\nrun-yarppg ui=qt6_simple\n</code></pre>"},{"location":"#more-to-come-you-are-welcome-to-contribute","title":"More to come, you are welcome to contribute","text":"<ol> <li> <p>W Verkruysse, L O Svaasand and J S Nelson. Remote plethysmographic imaging using ambient light. Optics Express. 2008;16(26):21434\u201321445. doi:10.1364/oe.16.021434 \u21a9</p> </li> </ol>"},{"location":"offline_processing/","title":"Offline processing","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Processing videos offline.\"\"\"\n</pre> \"\"\"Processing videos offline.\"\"\" In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\nimport yarppg\n</pre> import matplotlib.pyplot as plt import numpy as np  import yarppg In\u00a0[\u00a0]: Copied! <pre>filename = \"../tests/testvideo_30fps.mp4\"\n\nfps = yarppg.helpers.get_video_fps(filename)\nfilter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\")\nlivefilter = yarppg.digital_filter.make_digital_filter(filter_cfg)\nprocessor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter)\nrppg = yarppg.Rppg(\n    processor=processor, hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4)\n)\n</pre> filename = \"../tests/testvideo_30fps.mp4\"  fps = yarppg.helpers.get_video_fps(filename) filter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\") livefilter = yarppg.digital_filter.make_digital_filter(filter_cfg) processor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter) rppg = yarppg.Rppg(     processor=processor, hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4) ) In\u00a0[\u00a0]: Copied! <pre>results = rppg.process_video(filename)\n</pre> results = rppg.process_video(filename) In\u00a0[\u00a0]: Copied! <pre>values = np.array(results)\nhrs = yarppg.bpm_from_frames_per_beat(np.array([res.hr for res in results]), fps)\nplt.plot(results[int(2 * fps) :])\n</pre> values = np.array(results) hrs = yarppg.bpm_from_frames_per_beat(np.array([res.hr for res in results]), fps) plt.plot(results[int(2 * fps) :]) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"profile_processing/","title":"Profile processing","text":"In\u00a0[\u00a0]: Copied! <pre>\"\"\"Profile the offline video processing.\"\"\"\n</pre> \"\"\"Profile the offline video processing.\"\"\" In\u00a0[\u00a0]: Copied! <pre>#! %load_ext cProfile\nimport yarppg\n</pre> #! %load_ext cProfile import yarppg In\u00a0[\u00a0]: Copied! <pre>filename = \"../tests/testvideo_30fps.mp4\"\n\nfps = yarppg.helpers.get_video_fps(filename)\nfilter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\")\nlivefilter = yarppg.digital_filter.make_digital_filter(filter_cfg)\nprocessor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter)\nrppg = yarppg.Rppg(\n    processor=processor, hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4)\n)\n</pre> filename = \"../tests/testvideo_30fps.mp4\"  fps = yarppg.helpers.get_video_fps(filename) filter_cfg = yarppg.digital_filter.FilterConfig(fps, 0.5, 1.5, btype=\"bandpass\") livefilter = yarppg.digital_filter.make_digital_filter(filter_cfg) processor = yarppg.FilteredProcessor(yarppg.Processor(), livefilter=livefilter) rppg = yarppg.Rppg(     processor=processor, hr_calc=yarppg.PeakBasedHrCalculator(fps, window_seconds=4) ) In\u00a0[\u00a0]: Copied! <pre>#!%prun -D process_video.prof rppg.process_video(filename)\n</pre> #!%prun -D process_video.prof rppg.process_video(filename)"},{"location":"reference/hr_calculator/","title":"Heart rate estimation","text":"<p>Heart rate calculation utilities.</p>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.HrCalculator","title":"<code>HrCalculator</code>","text":"<p>Base class for heart rate calculation.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>class HrCalculator:\n    \"\"\"Base class for heart rate calculation.\"\"\"\n\n    def update(self, data: RppgResult) -&gt; float:  # noqa: ARG002\n        \"\"\"Process the new data and update HR estimate.\"\"\"\n        return np.nan\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.HrCalculator.update","title":"<code>update(data)</code>","text":"<p>Process the new data and update HR estimate.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>def update(self, data: RppgResult) -&gt; float:  # noqa: ARG002\n    \"\"\"Process the new data and update HR estimate.\"\"\"\n    return np.nan\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.PeakBasedHrCalculator","title":"<code>PeakBasedHrCalculator</code>","text":"<p>               Bases: <code>HrCalculator</code></p> <p>Peak-based heart rate calculation.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>class PeakBasedHrCalculator(HrCalculator):\n    \"\"\"Peak-based heart rate calculation.\"\"\"\n\n    def __init__(\n        self,\n        fs: float,\n        window_seconds: float = 10,\n        distance: float = 0.5,\n        update_interval: int = 10,\n    ):\n        self.winsize = int(fs * window_seconds)\n        self.values = deque(maxlen=self.winsize)\n        self.mindist = int(fs * distance)\n\n        self.update_interval = update_interval\n        self.frames_seen = 0\n        self.last_hr = np.nan\n\n    def update(self, data: RppgResult) -&gt; float:\n        \"\"\"Process the new data and update HR estimate in frames per beat.\"\"\"\n        self.frames_seen += 1\n        self.values.append(data.value)\n        if (\n            len(self.values) &lt; self.winsize\n            or self.frames_seen % self.update_interval != 0\n        ):\n            return self.last_hr\n        peaks, _ = scipy.signal.find_peaks(self.values, distance=self.mindist)\n        self.last_hr = np.diff(peaks).mean()\n        return self.last_hr\n</code></pre>"},{"location":"reference/hr_calculator/#yarppg.hr_calculator.PeakBasedHrCalculator.update","title":"<code>update(data)</code>","text":"<p>Process the new data and update HR estimate in frames per beat.</p> Source code in <code>src\\yarppg\\hr_calculator.py</code> <pre><code>def update(self, data: RppgResult) -&gt; float:\n    \"\"\"Process the new data and update HR estimate in frames per beat.\"\"\"\n    self.frames_seen += 1\n    self.values.append(data.value)\n    if (\n        len(self.values) &lt; self.winsize\n        or self.frames_seen % self.update_interval != 0\n    ):\n        return self.last_hr\n    peaks, _ = scipy.signal.find_peaks(self.values, distance=self.mindist)\n    self.last_hr = np.diff(peaks).mean()\n    return self.last_hr\n</code></pre>"},{"location":"reference/processors/","title":"rPPG processors","text":"<p>Implementations of various rPPG signal extractors found in literature.</p>"},{"location":"reference/rppg/","title":"The rPPG orchestrator","text":"<p>Provides the Rppg orchestrator class.</p> <p>The orchestrator ties together the typical steps required in an rPPG pipeline:</p> <ol> <li>region of interest (ROI) identification (yarppg.roi)</li> <li>rPPG signal extraction (yarppg.processors)</li> <li>heart rate estimation (yarppg.hr_calculator)</li> </ol> <p><code>Rppg</code>'s <code>process_frame</code> method performs the three steps from above in order and produces an yarppg.containers.RppgResult that holds the extracted rPPG signal value as well as the frame, ROI and some additional information.</p> <pre><code>import yarppg\n\ndefault_settings = yarppg.Settings()\nrppg = yarppg.Rppg.from_settings(default_settings)\n\nresult = rppg.process_frame(frame)  # input a (h x w x 3)-image array.\nprint(result.hr)\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg","title":"<code>Rppg</code>","text":"<p>Orchestrator for the complete rPPG pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>roi_detector</code> <code>RoiDetector | None</code> <p>detector for identifying the region of interest (and background).</p> <code>None</code> <code>processor</code> <code>Processor | None</code> <p>rPPG signal extraction algorithm.</p> <code>None</code> <code>hr_calc</code> <code>HrCalculator | None</code> <p>heart rate calculation algorithm.</p> <code>None</code> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>class Rppg:\n    \"\"\"Orchestrator for the complete rPPG pipeline.\n\n    Args:\n       roi_detector: detector for identifying the region of interest (and background).\n       processor: rPPG signal extraction algorithm.\n       hr_calc: heart rate calculation algorithm.\n    \"\"\"\n\n    def __init__(\n        self,\n        roi_detector: roi.RoiDetector | None = None,\n        processor: processors.Processor | None = None,\n        hr_calc: hr_calculator.HrCalculator | None = None,\n        fps: float = 30,\n    ):\n        self.roi_detector = roi_detector or roi.FaceMeshDetector()\n        self.processor = processor or processors.Processor()\n        self.hr_calculator = hr_calc or hr_calculator.PeakBasedHrCalculator(fps)\n\n    def process_frame(self, frame: np.ndarray) -&gt; RppgResult:\n        \"\"\"Process a single frame from video or live stream.\"\"\"\n        roi = self.roi_detector.detect(frame)\n        result = self.processor.process(frame, roi)\n        result.hr = self.hr_calculator.update(result)\n\n        return result\n\n    def process_video(self, filename: str | pathlib.Path) -&gt; list[RppgResult]:\n        \"\"\"Convenience function to process an entire video file at once.\"\"\"\n        results = []\n        for frame in helpers.frames_from_video(filename):\n            results.append(self.process_frame(frame))\n        return results\n\n    def reset(self) -&gt; None:\n        \"\"\"Reset internal elements.\"\"\"\n        self.processor.reset()\n\n    @classmethod\n    def from_settings(cls, settings: Settings) -&gt; \"Rppg\":\n        \"\"\"Instantiate rPPG orchestrator with the given settings.\"\"\"\n        detector = roi.detectors[settings.detector]()\n        processor = processors.algorithms[settings.algorithm]()\n        if settings.filter:\n            if settings.filter == \"bandpass\":\n                b, a = scipy.signal.iirfilter(2, [0.7, 1.8], fs=30, btype=\"band\")\n                livefilter = digital_filter.DigitalFilter(b, a)\n            else:\n                livefilter = digital_filter.make_digital_filter(settings.filter)\n            processor = processors.FilteredProcessor(processor, livefilter)\n        return cls(detector, processor)\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.from_settings","title":"<code>from_settings(settings)</code>  <code>classmethod</code>","text":"<p>Instantiate rPPG orchestrator with the given settings.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>@classmethod\ndef from_settings(cls, settings: Settings) -&gt; \"Rppg\":\n    \"\"\"Instantiate rPPG orchestrator with the given settings.\"\"\"\n    detector = roi.detectors[settings.detector]()\n    processor = processors.algorithms[settings.algorithm]()\n    if settings.filter:\n        if settings.filter == \"bandpass\":\n            b, a = scipy.signal.iirfilter(2, [0.7, 1.8], fs=30, btype=\"band\")\n            livefilter = digital_filter.DigitalFilter(b, a)\n        else:\n            livefilter = digital_filter.make_digital_filter(settings.filter)\n        processor = processors.FilteredProcessor(processor, livefilter)\n    return cls(detector, processor)\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.process_frame","title":"<code>process_frame(frame)</code>","text":"<p>Process a single frame from video or live stream.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>def process_frame(self, frame: np.ndarray) -&gt; RppgResult:\n    \"\"\"Process a single frame from video or live stream.\"\"\"\n    roi = self.roi_detector.detect(frame)\n    result = self.processor.process(frame, roi)\n    result.hr = self.hr_calculator.update(result)\n\n    return result\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.process_video","title":"<code>process_video(filename)</code>","text":"<p>Convenience function to process an entire video file at once.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>def process_video(self, filename: str | pathlib.Path) -&gt; list[RppgResult]:\n    \"\"\"Convenience function to process an entire video file at once.\"\"\"\n    results = []\n    for frame in helpers.frames_from_video(filename):\n        results.append(self.process_frame(frame))\n    return results\n</code></pre>"},{"location":"reference/rppg/#yarppg.rppg.Rppg.reset","title":"<code>reset()</code>","text":"<p>Reset internal elements.</p> Source code in <code>src\\yarppg\\rppg.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Reset internal elements.\"\"\"\n    self.processor.reset()\n</code></pre>"},{"location":"reference/roi/","title":"ROI detectors","text":"<p>Utilities for ROI (region of interest) detection and manipulation.</p>"},{"location":"reference/roi/facemesh_detector/","title":"MediaPipe FaceMesh","text":"<p>Detect the lower face with MediaPipe's FaceMesh detector.</p>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.FaceMeshDetector","title":"<code>FaceMeshDetector</code>","text":"<p>               Bases: <code>RoiDetector</code></p> <p>Face detector using MediaPipe's face landmarker.</p> <p>This detector is based on the face landmarker task from MediaPipe. https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/python</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>class FaceMeshDetector(RoiDetector):\n    \"\"\"Face detector using MediaPipe's face landmarker.\n\n    This detector is based on the face landmarker task from MediaPipe.\n    &lt;https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker/python&gt;\n    \"\"\"\n\n    _lower_face = [200, 431, 411, 340, 349, 120, 111, 187, 211]\n\n    def __init__(self, draw_landmarks=False, **kwargs):\n        super().__init__(**kwargs)\n        modelpath = get_face_landmarker_modelfile()\n        if modelpath is None:\n            raise FileNotFoundError(\"Could not find or download landmarker model file.\")\n        base_options = mp.tasks.BaseOptions(model_asset_path=modelpath)\n        landmarker_options = mp.tasks.vision.FaceLandmarkerOptions(\n            base_options=base_options,\n            running_mode=mp.tasks.vision.RunningMode.VIDEO,\n        )\n        self.landmarker = mp.tasks.vision.FaceLandmarker.create_from_options(\n            landmarker_options\n        )\n        self.draw_landmarks = draw_landmarks\n\n    def __del__(self):\n        self.landmarker.close()\n\n    def _process_landmarks(self, frame, results) -&gt; tuple[np.ndarray, np.ndarray]:\n        height, width = frame.shape[:2]\n        coords = get_landmark_coords(results.face_landmarks[0], width, height)[:, :2]\n        face_rect = get_boundingbox_from_coords(coords)\n\n        mask = contour_to_mask((height, width), coords[self._lower_face])\n        return mask, face_rect\n\n    def detect(self, frame: np.ndarray) -&gt; RegionOfInterest:\n        \"\"\"Find face landmarks and create ROI around the lower face region.\"\"\"\n        rawimg = frame.copy()\n        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            results = self.landmarker.detect_for_video(\n                mp_image, int(time.perf_counter() * 1000)\n            )\n\n        if len(results.face_landmarks) &lt; 1:\n            return RegionOfInterest(np.zeros_like(frame), baseimg=frame)\n\n        if self.draw_landmarks:\n            self.draw_facemesh(frame, results.face_landmarks[0], tesselate=True)\n\n        mask, face_rect = self._process_landmarks(frame, results)\n        return RegionOfInterest(mask, baseimg=rawimg, face_rect=tuple(face_rect))\n\n    def draw_facemesh(\n        self,\n        img,\n        face_landmarks,\n        tesselate=False,\n        contour=False,\n        irises=False,\n    ):\n        \"\"\"Draw the detected face landmarks on the image.\"\"\"\n        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()  # type: ignore\n        face_landmarks_proto.landmark.extend(\n            [\n                landmark_pb2.NormalizedLandmark(  # type: ignore\n                    x=landmark.x, y=landmark.y, z=landmark.z\n                )\n                for landmark in face_landmarks\n            ]\n        )\n        if tesselate:\n            mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n                image=img,\n                landmark_list=face_landmarks_proto,\n                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,  # type: ignore\n                landmark_drawing_spec=None,\n                connection_drawing_spec=TESSELATION_SPEC,\n            )\n        if contour:\n            mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n                image=img,\n                landmark_list=face_landmarks_proto,\n                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,  # type: ignore\n                landmark_drawing_spec=None,\n                connection_drawing_spec=CONTOUR_SPEC,\n            )\n        if irises:\n            mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n                image=img,\n                landmark_list=face_landmarks_proto,\n                connections=mp.solutions.face_mesh.FACEMESH_IRISES,  # type: ignore\n                landmark_drawing_spec=None,\n                connection_drawing_spec=IRISES_SPEC,\n            )\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.FaceMeshDetector.detect","title":"<code>detect(frame)</code>","text":"<p>Find face landmarks and create ROI around the lower face region.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def detect(self, frame: np.ndarray) -&gt; RegionOfInterest:\n    \"\"\"Find face landmarks and create ROI around the lower face region.\"\"\"\n    rawimg = frame.copy()\n    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        results = self.landmarker.detect_for_video(\n            mp_image, int(time.perf_counter() * 1000)\n        )\n\n    if len(results.face_landmarks) &lt; 1:\n        return RegionOfInterest(np.zeros_like(frame), baseimg=frame)\n\n    if self.draw_landmarks:\n        self.draw_facemesh(frame, results.face_landmarks[0], tesselate=True)\n\n    mask, face_rect = self._process_landmarks(frame, results)\n    return RegionOfInterest(mask, baseimg=rawimg, face_rect=tuple(face_rect))\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.FaceMeshDetector.draw_facemesh","title":"<code>draw_facemesh(img, face_landmarks, tesselate=False, contour=False, irises=False)</code>","text":"<p>Draw the detected face landmarks on the image.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def draw_facemesh(\n    self,\n    img,\n    face_landmarks,\n    tesselate=False,\n    contour=False,\n    irises=False,\n):\n    \"\"\"Draw the detected face landmarks on the image.\"\"\"\n    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()  # type: ignore\n    face_landmarks_proto.landmark.extend(\n        [\n            landmark_pb2.NormalizedLandmark(  # type: ignore\n                x=landmark.x, y=landmark.y, z=landmark.z\n            )\n            for landmark in face_landmarks\n        ]\n    )\n    if tesselate:\n        mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n            image=img,\n            landmark_list=face_landmarks_proto,\n            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,  # type: ignore\n            landmark_drawing_spec=None,\n            connection_drawing_spec=TESSELATION_SPEC,\n        )\n    if contour:\n        mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n            image=img,\n            landmark_list=face_landmarks_proto,\n            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,  # type: ignore\n            landmark_drawing_spec=None,\n            connection_drawing_spec=CONTOUR_SPEC,\n        )\n    if irises:\n        mp.solutions.drawing_utils.draw_landmarks(  # type: ignore\n            image=img,\n            landmark_list=face_landmarks_proto,\n            connections=mp.solutions.face_mesh.FACEMESH_IRISES,  # type: ignore\n            landmark_drawing_spec=None,\n            connection_drawing_spec=IRISES_SPEC,\n        )\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.get_boundingbox_from_coords","title":"<code>get_boundingbox_from_coords(coords)</code>","text":"<p>Calculate the bounding rectangle containing all landmarks.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def get_boundingbox_from_coords(coords: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Calculate the bounding rectangle containing all landmarks.\"\"\"\n    xy = np.min(coords, axis=0)\n    wh = np.subtract(np.max(coords, axis=0), xy)\n\n    return np.r_[xy, wh]\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.get_face_landmarker_modelfile","title":"<code>get_face_landmarker_modelfile()</code>","text":"<p>Get the filename of the FaceLandmarker - download file if necessary.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def get_face_landmarker_modelfile():\n    \"\"\"Get the filename of the FaceLandmarker - download file if necessary.\"\"\"\n    task_filename = \"face_landmarker.task\"\n    return get_cached_resource_path(\n        task_filename, MEDIAPIPE_MODELS_BASE + LANDMARKER_TASK\n    )\n</code></pre>"},{"location":"reference/roi/facemesh_detector/#yarppg.roi.facemesh_segmenter.get_landmark_coords","title":"<code>get_landmark_coords(landmarks, width, height)</code>","text":"<p>Extract normalized landmark coordinates to array of pixel coordinates.</p> Source code in <code>src\\yarppg\\roi\\facemesh_segmenter.py</code> <pre><code>def get_landmark_coords(\n    landmarks: list[landmark_module.NormalizedLandmark], width: int, height: int\n) -&gt; np.ndarray:\n    \"\"\"Extract normalized landmark coordinates to array of pixel coordinates.\"\"\"\n    xyz = [(lm.x, lm.y, lm.z) for lm in landmarks]\n    return np.multiply(xyz, [width, height, width]).astype(int)\n</code></pre>"}]}